{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb06d188",
   "metadata": {},
   "source": [
    "# Transformer (DistilBERT) Hyperparameter Tuning with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb8b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 14:37:47.250196: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-13 14:37:47.304977: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-13 14:37:56.191752: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n",
      "GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, DistilBertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPUs: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d36e4",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455b8150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train: 40000, Test: 10000\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LEN = 250  # Fixed\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "print(\"Loading data...\")\n",
    "dataset = pd.read_csv('data/imdb_dataset.csv')\n",
    "train_df, test_df = train_test_split(dataset, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "y_train = (train_df['sentiment'] == 'positive').astype(int).values\n",
    "y_test = (test_df['sentiment'] == 'positive').astype(int).values\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1eb23",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0193a83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing with MAX_LEN=250...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765629564.966494   10703 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5518 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizing with MAX_LEN={MAX_LEN}...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "X_train = tokenizer(\n",
    "    train_df['review'].tolist(),\n",
    "    max_length=MAX_LEN,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "X_test = tokenizer(\n",
    "    test_df['review'].tolist(),\n",
    "    max_length=MAX_LEN,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de9ec9",
   "metadata": {},
   "source": [
    "## Keras Tuner Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13990faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Tuner imported\n"
     ]
    }
   ],
   "source": [
    "# !pip install keras-tuner\n",
    "\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import BayesianOptimization\n",
    "from tf_keras.callbacks import EarlyStopping\n",
    "\n",
    "print(\"Keras Tuner imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371f280",
   "metadata": {},
   "source": [
    "## Tunable Transformer Model Builder\n",
    "\n",
    "Tuning: Learning Rate, Dropout, Weight Decay, Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736af49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model builder ready\n"
     ]
    }
   ],
   "source": [
    "def build_tunable_transformer(hp):\n",
    "    # Hyperparameter: Learning rate (log scale)\n",
    "    learning_rate = hp.Float(\n",
    "        'learning_rate',\n",
    "        min_value=1e-5,\n",
    "        max_value=5e-5,\n",
    "        sampling='log',\n",
    "        default=2e-5\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter: Dropout (transformer internal dropout)\n",
    "    dropout = hp.Float(\n",
    "        'dropout',\n",
    "        min_value=0.05,\n",
    "        max_value=0.3,\n",
    "        step=0.05,\n",
    "        default=0.1\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter: Attention dropout\n",
    "    attention_dropout = hp.Float(\n",
    "        'attention_dropout',\n",
    "        min_value=0.05,\n",
    "        max_value=0.3,\n",
    "        step=0.05,\n",
    "        default=0.1\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter: Weight decay (L2 regularization)\n",
    "    weight_decay = hp.Float(\n",
    "        'weight_decay',\n",
    "        min_value=0.001,\n",
    "        max_value=0.02,\n",
    "        step=0.001,\n",
    "        default=0.01\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter: Batch size\n",
    "    batch_size = hp.Choice(\n",
    "        'batch_size',\n",
    "        values=[8, 16, 32],\n",
    "        default=16\n",
    "    )\n",
    "    \n",
    "    # Store batch size for training\n",
    "    hp.values['_batch_size'] = batch_size\n",
    "    \n",
    "    # Create custom config with dropout\n",
    "    config = DistilBertConfig.from_pretrained(MODEL_NAME)\n",
    "    config.dropout = dropout\n",
    "    config.attention_dropout = attention_dropout\n",
    "    config.num_labels = 2  # Set num_labels in config\n",
    "    \n",
    "    # Load model with custom config\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=config,\n",
    "        use_safetensors=False\n",
    "    )\n",
    "    \n",
    "    # Compile with AdamW (Adam with weight decay) - use tf_keras optimizer\n",
    "    optimizer = tf_keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf_keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Model builder ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e7435",
   "metadata": {},
   "source": [
    "## Configure Bayesian Optimization Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4f61860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST MODE: Tuner configured for 2 trials only\n",
      "Tuning: LEARNING_RATE, DROPOUT, ATTENTION_DROPOUT, WEIGHT_DECAY, BATCH_SIZE\n",
      "To run full search: Change max_trials=2 to max_trials=30\n"
     ]
    }
   ],
   "source": [
    "# Custom tuner to handle variable batch size\n",
    "class TransformerTuner(BayesianOptimization):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "        \n",
    "        # Build model\n",
    "        model = self.hypermodel.build(hp)\n",
    "        \n",
    "        # Get batch size\n",
    "        batch_size = hp.values.get('_batch_size', 16)\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            {'input_ids': X_train['input_ids'], 'attention_mask': X_train['attention_mask']},\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=kwargs.get('epochs', 3),\n",
    "            validation_data=(\n",
    "                {'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']},\n",
    "                y_test\n",
    "            ),\n",
    "            verbose=kwargs.get('verbose', 1),\n",
    "            callbacks=kwargs.get('callbacks', [])\n",
    "        )\n",
    "        \n",
    "        return max(history.history['val_accuracy'])\n",
    "\n",
    "# TEST MODE: Only 2 trials (change to 30 for full run)\n",
    "tuner = TransformerTuner(\n",
    "    hypermodel=build_tunable_transformer,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=2,  # TEST: 2 trials only (change to 30 for full search)\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_results',\n",
    "    project_name='transformer_tuning_test',  # Different project name for test\n",
    "    overwrite=True,  # Overwrite test runs\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"TEST MODE: Tuner configured for 2 trials only\")\n",
    "print(\"Tuning: LEARNING_RATE, DROPOUT, ATTENTION_DROPOUT, WEIGHT_DECAY, BATCH_SIZE\")\n",
    "print(\"To run full search: Change max_trials=2 to max_trials=30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20422b54",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cdde291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping configured (patience=1)\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=1,\n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Early stopping configured (patience=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd100d7",
   "metadata": {},
   "source": [
    "## Execute Hyperparameter Search\n",
    "\n",
    "**TEST MODE:** Running 2 trials (~10-20 minutes)\n",
    "\n",
    "For full search: Change max_trials to 30 (3-6 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6f58ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 20s]\n",
      "\n",
      "Best val_accuracy So Far: None\n",
      "Total elapsed time: 00h 00m 20s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "1.7617e-05        |2.7986e-05        |learning_rate\n",
      "0.15              |0                 |dropout\n",
      "0                 |0.1               |attention_dropout\n",
      "0.005             |0.002             |weight_decay\n",
      "8                 |32                |batch_size\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 14:40:10.469070: I external/local_xla/xla/service/service.cc:163] XLA service 0x7cce51ebc430 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-13 14:40:10.469100: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2025-12-13 14:40:10.486218: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-13 14:40:10.531811: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91700\n",
      "I0000 00:00:1765629610.632305   10795 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 641/5000 [==>...........................] - ETA: 9:11 - loss: 0.3840 - accuracy: 0.8264"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST MODE: TRANSFORMER HYPERPARAMETER SEARCH (2 trials)\")\n",
    "print(f\"Training set: {len(train_df)}, Test set: {len(test_df)}\")\n",
    "print(\"Max epochs per trial: 3\")\n",
    "print(\"Estimated time: 10-20 minutes\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "search_start = time.time()\n",
    "\n",
    "tuner.search(\n",
    "    epochs=3,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search_time = time.time() - search_start\n",
    "print(f\"\\nSearch complete: {search_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea7eaf6",
   "metadata": {},
   "source": [
    "## Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BEST HYPERPARAMETERS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(f\"Learning Rate: {best_hps.get('learning_rate'):.6f}\")\n",
    "print(f\"Dropout: {best_hps.get('dropout'):.3f}\")\n",
    "print(f\"Attention Dropout: {best_hps.get('attention_dropout'):.3f}\")\n",
    "print(f\"Weight Decay: {best_hps.get('weight_decay'):.4f}\")\n",
    "print(f\"Batch Size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"\\nComparison with original:\")\n",
    "print(f\"  LR: 2e-05 -> {best_hps.get('learning_rate'):.6f}\")\n",
    "print(f\"  Dropout: 0.1 -> {best_hps.get('dropout'):.3f}\")\n",
    "print(f\"  Batch: 16 -> {best_hps.get('batch_size')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a684a",
   "metadata": {},
   "source": [
    "## Top 5 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOP 5 CONFIGURATIONS:\")\n",
    "\n",
    "for i, trial in enumerate(tuner.oracle.get_best_trials(5), 1):\n",
    "    hp = trial.hyperparameters\n",
    "    print(f\"\\n#{i}: Accuracy={trial.score:.4f}\")\n",
    "    print(f\"  LR={hp.get('learning_rate'):.6f}, Dropout={hp.get('dropout'):.2f}, \")\n",
    "    print(f\"  AttnDrop={hp.get('attention_dropout'):.2f}, WeightDecay={hp.get('weight_decay'):.4f}, Batch={hp.get('batch_size')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93cc36",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f88b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trial_data = []\n",
    "for trial in tuner.oracle.trials.values():\n",
    "    if trial.score is not None:\n",
    "        hp = trial.hyperparameters\n",
    "        trial_data.append({\n",
    "            'trial_id': trial.trial_id,\n",
    "            'val_accuracy': trial.score,\n",
    "            'learning_rate': hp.get('learning_rate'),\n",
    "            'dropout': hp.get('dropout'),\n",
    "            'attention_dropout': hp.get('attention_dropout'),\n",
    "            'weight_decay': hp.get('weight_decay'),\n",
    "            'batch_size': hp.get('batch_size')\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(trial_data)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Transformer Hyperparameter Tuning Results', fontsize=16)\n",
    "\n",
    "# 1. Progress\n",
    "axes[0,0].plot(df['trial_id'], df['val_accuracy'], 'o-')\n",
    "axes[0,0].set_title('Accuracy Progress')\n",
    "axes[0,0].set_xlabel('Trial')\n",
    "axes[0,0].set_ylabel('Validation Accuracy')\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Learning Rate\n",
    "axes[0,1].scatter(df['learning_rate'], df['val_accuracy'], s=100, alpha=0.6)\n",
    "axes[0,1].set_xscale('log')\n",
    "axes[0,1].set_title('Learning Rate vs Accuracy')\n",
    "axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Dropout\n",
    "axes[0,2].scatter(df['dropout'], df['val_accuracy'], s=100, alpha=0.6)\n",
    "axes[0,2].set_title('Dropout vs Accuracy')\n",
    "axes[0,2].grid(alpha=0.3)\n",
    "\n",
    "# 4. Attention Dropout\n",
    "axes[1,0].scatter(df['attention_dropout'], df['val_accuracy'], s=100, alpha=0.6)\n",
    "axes[1,0].set_title('Attention Dropout vs Accuracy')\n",
    "axes[1,0].grid(alpha=0.3)\n",
    "\n",
    "# 5. Weight Decay\n",
    "axes[1,1].scatter(df['weight_decay'], df['val_accuracy'], s=100, alpha=0.6)\n",
    "axes[1,1].set_title('Weight Decay vs Accuracy')\n",
    "axes[1,1].grid(alpha=0.3)\n",
    "\n",
    "# 6. Batch Size\n",
    "df.groupby('batch_size')['val_accuracy'].agg(['mean', 'max']).plot(kind='bar', ax=axes[1,2])\n",
    "axes[1,2].set_title('Performance by Batch Size')\n",
    "axes[1,2].set_xticklabels(axes[1,2].get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest: {df['val_accuracy'].max():.4f}, Mean: {df['val_accuracy'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaedbb8f",
   "metadata": {},
   "source": [
    "## Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training final model with best hyperparameters...\")\n",
    "\n",
    "# Get best config\n",
    "config = DistilBertConfig.from_pretrained(MODEL_NAME)\n",
    "config.dropout = best_hps.get('dropout')\n",
    "config.attention_dropout = best_hps.get('attention_dropout')\n",
    "config.num_labels = 2  # Set num_labels in config\n",
    "\n",
    "final_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=config,\n",
    "    use_safetensors=False\n",
    ")\n",
    "\n",
    "final_optimizer = tf_keras.optimizers.AdamW(\n",
    "    learning_rate=best_hps.get('learning_rate'),\n",
    "    weight_decay=best_hps.get('weight_decay')\n",
    ")\n",
    "\n",
    "final_model.compile(\n",
    "    optimizer=final_optimizer,\n",
    "    loss=tf_keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "final_early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)\n",
    "\n",
    "final_history = final_model.fit(\n",
    "    {'input_ids': X_train['input_ids'], 'attention_mask': X_train['attention_mask']},\n",
    "    y_train,\n",
    "    batch_size=best_hps.get('batch_size'),\n",
    "    epochs=5,\n",
    "    validation_data=(\n",
    "        {'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']},\n",
    "        y_test\n",
    "    ),\n",
    "    callbacks=[final_early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "final_loss, final_acc = final_model.evaluate(\n",
    "    {'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']},\n",
    "    y_test,\n",
    "    batch_size=best_hps.get('batch_size')\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Accuracy: {final_acc:.4f}\")\n",
    "print(f\"Final Loss: {final_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3c441",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec357ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "results = {\n",
    "    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'model': 'DistilBERT',\n",
    "    'max_length': 250,\n",
    "    'total_trials': len(df),\n",
    "    'best_hyperparameters': {\n",
    "        'learning_rate': float(best_hps.get('learning_rate')),\n",
    "        'dropout': float(best_hps.get('dropout')),\n",
    "        'attention_dropout': float(best_hps.get('attention_dropout')),\n",
    "        'weight_decay': float(best_hps.get('weight_decay')),\n",
    "        'batch_size': int(best_hps.get('batch_size'))\n",
    "    },\n",
    "    'final_accuracy': float(final_acc),\n",
    "    'final_loss': float(final_loss)\n",
    "}\n",
    "\n",
    "with open('transformer_tuning_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "final_model.save_pretrained('models/distilbert_tuned')\n",
    "tokenizer.save_pretrained('models/distilbert_tuned')\n",
    "df.to_csv('transformer_tuning_history.csv', index=False)\n",
    "\n",
    "print('Saved: transformer_tuning_results.json')\n",
    "print('Saved: models/distilbert_tuned/')\n",
    "print('Saved: transformer_tuning_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c14e50",
   "metadata": {},
   "source": [
    "## Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07baf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "    \"This was the best movie I have ever seen!\",\n",
    "    \"I really hated this film. It was slow and boring.\"\n",
    "]\n",
    "\n",
    "test_enc = tokenizer(test_texts, max_length=250, truncation=True, padding=True, return_tensors='tf')\n",
    "preds = final_model.predict({'input_ids': test_enc['input_ids'], 'attention_mask': test_enc['attention_mask']})\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for text, pred in zip(test_texts, preds.logits):\n",
    "    sentiment = \"Positive\" if pred[1] > pred[0] else \"Negative\"\n",
    "    conf = tf.nn.softmax(pred).numpy().max()\n",
    "    print(f\"{text[:50]}... -> {sentiment} ({conf:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
