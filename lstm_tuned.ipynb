{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T19:00:56.579801Z",
     "start_time": "2025-11-08T19:00:56.006270Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 01:35:52.328956: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-13 01:35:52.810397: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-13 01:36:02.546256: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from keras.src.layers import Bidirectional\n",
    "from tensorflow.keras import mixed_precision\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('data/imdb_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a6c87f1339de63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T19:01:08.575056Z",
     "start_time": "2025-11-08T19:00:56.584886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe vectors from data/glove.6B.100d.txt...\n",
      "Successfully loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Define GloVe parameters\n",
    "GLOVE_FILE = 'data/glove.6B.100d.txt'\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "#Load GloVe vectors into a dictionary\n",
    "embeddings_index = {}\n",
    "print(f\"Loading GloVe vectors from {GLOVE_FILE}...\")\n",
    "\n",
    "try:\n",
    "    with open(GLOVE_FILE, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n*** ERROR: GloVe file not found. ***\")\n",
    "    print(f\"Please download '{GLOVE_FILE}' and place it in your project folder.\")\n",
    "\n",
    "print(f\"Successfully loaded {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8bd2db7067d1e",
   "metadata": {},
   "source": [
    "Split the original DataFrame into train and test sets\n",
    "80% for training, 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc3c115ed488112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T19:01:08.832785Z",
     "start_time": "2025-11-08T19:01:08.820252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 50000\n",
      "Training set size: 40000\n",
      "Test set size: 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39087</th>\n",
       "      <td>That's what I kept asking myself during the ma...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30893</th>\n",
       "      <td>I did not watch the entire movie. I could not ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45278</th>\n",
       "      <td>A touching love story reminiscent of In the M...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16398</th>\n",
       "      <td>This latter-day Fulci schlocker is a totally a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653</th>\n",
       "      <td>First of all, I firmly believe that Norwegian ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13748</th>\n",
       "      <td>I don't know how this movie received so many p...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23965</th>\n",
       "      <td>Nightmare Weekend stars a cast of ridiculous a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45552</th>\n",
       "      <td>:::SPOILER ALERT:::&lt;br /&gt;&lt;br /&gt;Soooo, Arnie's ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30219</th>\n",
       "      <td>The people who are bad-mouthing this film are ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24079</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;As usual, I was really looking for...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "39087  That's what I kept asking myself during the ma...  negative\n",
       "30893  I did not watch the entire movie. I could not ...  negative\n",
       "45278  A touching love story reminiscent of In the M...  positive\n",
       "16398  This latter-day Fulci schlocker is a totally a...  negative\n",
       "13653  First of all, I firmly believe that Norwegian ...  negative\n",
       "13748  I don't know how this movie received so many p...  negative\n",
       "23965  Nightmare Weekend stars a cast of ridiculous a...  negative\n",
       "45552  :::SPOILER ALERT:::<br /><br />Soooo, Arnie's ...  negative\n",
       "30219  The people who are bad-mouthing this film are ...  positive\n",
       "24079  <br /><br />As usual, I was really looking for...  negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Full dataset size: {len(dataset)}\")\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333541e948bd815a",
   "metadata": {},
   "source": [
    "Defining dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66eda303481e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8000\n",
    "MAX_LEN = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315073ba2d1665b",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7977b1bdfcfc5ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- First 10 rows of processed X_train (numbers) ---\n",
      "[[  13   92  485 ...  205  351 3856]\n",
      " [ 438   17   10 ...   89  103    9]\n",
      " [   0    0    0 ...    2  710   62]\n",
      " ...\n",
      " [  47 1025    8 ...  661  335  155]\n",
      " [   0    0    0 ...   17   63    6]\n",
      " [   5  131   71 ...  483    7    7]]\n",
      "\n",
      "--- Full 1st row of processed X_train ---\n",
      "[  13   92  485 6830   15    3  364 1182   61    8    1  470  216 1014\n",
      "    5 4160    8    3  174    4   34  440  697  623   12 3748  237  111\n",
      "  848   35  170   30  219  197    1  428  367   55 3765    3  278    7\n",
      "    7  157 1707  187    6    1  727 1935    1 1200    4 2946 3749 1828\n",
      "    2  147  144    3  228    4    3  207  323    2  144 1083   16   88\n",
      "    4  132 2871   18   10  153   99    4    1 4020  302   11   17 1001\n",
      "   35    1  496  492 2619  249   71   77  107  107  698   60   86 1047\n",
      " 1363    5  229  132   23 4360   31  138  209 1154   14 4501 5313   31\n",
      "    3 2386    2    8   11    6    3  445   14  624    4    1  718 2959\n",
      "    1 1278    2   71 3616    1  166 1507    1 1245    5 1629    1  879\n",
      " 1268    5    1  310  140 2894    2  410  633    7    7    1  269    6\n",
      " 3553 1000    5   26   39   14 1381  217   65    2   46    6   30  219\n",
      "   27  193 1484    8 1101   18   10 4905   84    1  226   66  356   68\n",
      "   54   27    5 3600   15   44   21  192    5    3  889 3511 1758   22\n",
      "   25    5  158  196  175    3  110   12 1652  471   75  221    5  325\n",
      "    2 3512   36   23   50   72 1889 4729   14    9 1393   11   19    6\n",
      "    3 3554 2029   16   61    1 2436  470  532    2    3  171 2577 2761\n",
      " 1778    5  585    9   35    1 3797  450  155  205  351 3856]\n"
     ]
    }
   ],
   "source": [
    "# Create the 0/1 labels\n",
    "y_train = train_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
    "y_test = test_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
    "\n",
    "# Process Reviews (X data)\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "\n",
    "#Fit the tokenizer ONLY on the training text\n",
    "tokenizer.fit_on_texts(train_df['review'])\n",
    "\n",
    "# transform both train and test text\n",
    "X_train = tokenizer.texts_to_sequences(train_df['review'])\n",
    "X_test = tokenizer.texts_to_sequences(test_df['review'])\n",
    "\n",
    "#Pad the sequences\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_LEN)\n",
    "\n",
    "print(\"\\n--- First 10 rows of processed X_train (numbers) ---\")\n",
    "print(X_train[:10])\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print(\"\\n--- Full 1st row of processed X_train ---\")\n",
    "print(X_train[0])\n",
    "np.set_printoptions(threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7cc1b230cd43a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding matrix for top 8000 words...\n",
      "Done. Found 7858 / 8000 words in GloVe.\n"
     ]
    }
   ],
   "source": [
    "# Get your tokenizer's vocabulary\n",
    "VOCAB_SIZE_FINAL = tokenizer.num_words + 1\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE_FINAL, EMBEDDING_DIM))\n",
    "words_found = 0\n",
    "print(f\"Building embedding matrix for top {VOCAB_SIZE} words...\")\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= VOCAB_SIZE_FINAL:\n",
    "        continue\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        words_found += 1\n",
    "\n",
    "print(f\"Done. Found {words_found} / {VOCAB_SIZE} words in GloVe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635e50a",
   "metadata": {},
   "source": [
    "## Keras Tuner Setup for Automatic Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28212f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Tuner imported\n"
     ]
    }
   ],
   "source": [
    "# Install Keras Tuner (run once)\n",
    "# !pip install keras-tuner\n",
    "\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import BayesianOptimization\n",
    "from keras.src.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(\"Keras Tuner imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8923eb",
   "metadata": {},
   "source": [
    "## Tunable Model Builder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bead8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model builder ready\n"
     ]
    }
   ],
   "source": [
    "def build_tunable_model(hp):\n",
    "    num_layers = hp.Int(\"num_lstm_layers\", min_value=1, max_value=3, default=2)\n",
    "    lstm_units = hp.Choice(\"lstm_units\", values=[50, 64, 100, 128, 192, 256])\n",
    "    dropout = hp.Float(\"dropout\", min_value=0.0, max_value=0.5, step=0.05, default=0.12)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"log\", default=5e-5)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=VOCAB_SIZE_FINAL, output_dim=EMBEDDING_DIM, weights=[embedding_matrix], trainable=True, input_length=MAX_LEN))\n",
    "    model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout, return_sequences=(num_layers > 1))))\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        return_seq = (i < num_layers - 1)\n",
    "        model.add(LSTM(units=lstm_units, dropout=dropout, return_sequences=return_seq))\n",
    "    \n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "print(\"Model builder ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfaa857",
   "metadata": {},
   "source": [
    "## Configure Bayesian Optimization Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84047b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_results/lstm_sentiment_tuning/tuner0.json\n",
      "Tuner configured: 75 trials, Bayesian Optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765582592.718079   24466 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5518 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "tuner = BayesianOptimization(\n",
    "    hypermodel=build_tunable_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=75,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"tuner_results\",\n",
    "    project_name=\"lstm_sentiment_tuning\",\n",
    "    overwrite=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Tuner configured: 75 trials, Bayesian Optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56aa31",
   "metadata": {},
   "source": [
    "## Early Stopping Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b04391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks configured\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True, mode=\"max\", verbose=1)\n",
    "\n",
    "class TrialPruning(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, min_accuracy=0.70):\n",
    "        super().__init__()\n",
    "        self.min_accuracy = min_accuracy\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1 and logs.get(\"val_accuracy\", 0) < self.min_accuracy:\n",
    "            print(f\"Stopping trial: val_accuracy {logs['val_accuracy']:.4f} < {self.min_accuracy}\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "trial_pruning = TrialPruning(min_accuracy=0.70)\n",
    "tuner_callbacks = [early_stopping, trial_pruning]\n",
    "print(\"Callbacks configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1195fa",
   "metadata": {},
   "source": [
    "## Execute Hyperparameter Search (3-6 hours for 75 trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f94706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 36 Complete [00h 03m 39s]\n",
      "val_accuracy: 0.9111999869346619\n",
      "\n",
      "Best val_accuracy So Far: 0.9132999777793884\n",
      "Total elapsed time: 11h 36m 58s\n",
      "\n",
      "Search: Running Trial #37\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |1                 |num_lstm_layers\n",
      "50                |256               |lstm_units\n",
      "0                 |0.45              |dropout\n",
      "0.00020064        |0.001             |learning_rate\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/minda/PycharmProjects/ImdbSentimentAnalysis/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.6757 - loss: 0.5844 - val_accuracy: 0.7785 - val_loss: 0.4675\n",
      "Epoch 2/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 71ms/step - accuracy: 0.8180 - loss: 0.4102 - val_accuracy: 0.8351 - val_loss: 0.3777\n",
      "Epoch 3/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 70ms/step - accuracy: 0.8433 - loss: 0.3626 - val_accuracy: 0.8455 - val_loss: 0.3531\n",
      "Epoch 4/20\n",
      "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8595 - loss: 0.3339"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING HYPERPARAMETER SEARCH\")\n",
    "print(f\"Training set: {len(X_train)}, Validation set: {len(X_test)}\")\n",
    "print(\"Max epochs per trial: 20\")\n",
    "print(\"Estimated time: 3-6 hours\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "search_start = time.time()\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=tuner_callbacks, batch_size=256, verbose=1)\n",
    "\n",
    "search_time = time.time() - search_start\n",
    "print(f\"\\nSearch complete: {search_time/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90015f",
   "metadata": {},
   "source": [
    "## Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8498d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BEST HYPERPARAMETERS:\")\n",
    "print(\"=\"*80)\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(f\"LSTM Layers: {best_hps.get('num_lstm_layers')}\")\n",
    "print(f\"LSTM Units: {best_hps.get('lstm_units')}\")\n",
    "print(f\"Dropout: {best_hps.get('dropout'):.3f}\")\n",
    "print(f\"Learning Rate: {best_hps.get('learning_rate'):.6f}\")\n",
    "print(\"\\nComparison with original (2 layers, 100 units, 0.12 dropout, 0.00005 LR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abba08",
   "metadata": {},
   "source": [
    "## Top 5 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d023f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOP 5 CONFIGURATIONS:\")\n",
    "for i, trial in enumerate(tuner.oracle.get_best_trials(5), 1):\n",
    "    print(f\"\\n#{i}: Accuracy={trial.score:.4f}\")\n",
    "    print(f\"  Layers={trial.hyperparameters.get('num_lstm_layers')}, Units={trial.hyperparameters.get('lstm_units')}, Dropout={trial.hyperparameters.get('dropout'):.3f}, LR={trial.hyperparameters.get('learning_rate'):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580b995",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd225b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "trial_data = []\n",
    "for trial in tuner.oracle.trials.values():\n",
    "    if trial.score is not None:\n",
    "        trial_data.append({\n",
    "            \"trial_id\": trial.trial_id,\n",
    "            \"val_accuracy\": trial.score,\n",
    "            \"num_layers\": trial.hyperparameters.get(\"num_lstm_layers\"),\n",
    "            \"lstm_units\": trial.hyperparameters.get(\"lstm_units\"),\n",
    "            \"dropout\": trial.hyperparameters.get(\"dropout\"),\n",
    "            \"learning_rate\": trial.hyperparameters.get(\"learning_rate\")\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(trial_data)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle(\"Hyperparameter Tuning Results\", fontsize=16)\n",
    "\n",
    "axes[0,0].plot(df[\"trial_id\"], df[\"val_accuracy\"], \"o-\")\n",
    "axes[0,0].axhline(0.8885, color=\"red\", linestyle=\"--\", label=\"Original\")\n",
    "axes[0,0].set_title(\"Accuracy Progress\")\n",
    "axes[0,0].legend()\n",
    "\n",
    "axes[0,1].scatter(df[\"lstm_units\"], df[\"val_accuracy\"], c=df[\"num_layers\"], cmap=\"viridis\", s=100, alpha=0.6)\n",
    "axes[0,1].set_title(\"LSTM Units vs Accuracy\")\n",
    "\n",
    "axes[0,2].scatter(df[\"dropout\"], df[\"val_accuracy\"], c=df[\"lstm_units\"], cmap=\"plasma\", s=100, alpha=0.6)\n",
    "axes[0,2].set_title(\"Dropout vs Accuracy\")\n",
    "\n",
    "axes[1,0].scatter(df[\"learning_rate\"], df[\"val_accuracy\"], s=100, alpha=0.6)\n",
    "axes[1,0].set_xscale(\"log\")\n",
    "axes[1,0].set_title(\"Learning Rate vs Accuracy\")\n",
    "\n",
    "df.groupby(\"num_layers\")[\"val_accuracy\"].agg([\"mean\", \"max\"]).plot(kind=\"bar\", ax=axes[1,1])\n",
    "axes[1,1].set_title(\"Performance by Layers\")\n",
    "\n",
    "axes[1,2].text(0.5, 0.5, f\"Best: {df['val_accuracy'].max():.4f}\\nMean: {df['val_accuracy'].mean():.4f}\\nStd: {df['val_accuracy'].std():.4f}\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "axes[1,2].set_title(\"Summary Stats\")\n",
    "axes[1,2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Improvement over original: {(df['val_accuracy'].max() - 0.8885)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef16f5d",
   "metadata": {},
   "source": [
    "## Train Final Model with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbf210",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training final model with best hyperparameters...\")\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "final_early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True, mode=\"max\", verbose=1)\n",
    "\n",
    "final_history = best_model.fit(X_train, y_train, batch_size=256, epochs=50, validation_data=(X_test, y_test), callbacks=[final_early_stopping], verbose=1)\n",
    "\n",
    "final_score, final_acc = best_model.evaluate(X_test, y_test, batch_size=256)\n",
    "print(f\"\\nFinal Test Accuracy: {final_acc:.4f}\")\n",
    "print(f\"Final Test Loss: {final_score:.4f}\")\n",
    "print(f\"Improvement: {(final_acc - 0.8885)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb2ce2",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "results = {\n",
    "    \"date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"total_trials\": len(df),\n",
    "    \"best_hyperparameters\": {\n",
    "        \"num_lstm_layers\": int(best_hps.get(\"num_lstm_layers\")),\n",
    "        \"lstm_units\": int(best_hps.get(\"lstm_units\")),\n",
    "        \"dropout\": float(best_hps.get(\"dropout\")),\n",
    "        \"learning_rate\": float(best_hps.get(\"learning_rate\"))\n",
    "    },\n",
    "    \"final_accuracy\": float(final_acc),\n",
    "    \"final_loss\": float(final_score),\n",
    "    \"improvement\": float((final_acc - 0.8885) * 100)\n",
    "}\n",
    "\n",
    "with open(\"tuning_results_summary.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "best_model.save(\"models/lstm_tuned_best.keras\")\n",
    "df.to_csv(\"tuning_history.csv\", index=False)\n",
    "\n",
    "print(\"Saved: tuning_results_summary.json\")\n",
    "print(\"Saved: models/lstm_tuned_best.keras\")\n",
    "print(\"Saved: tuning_history.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db106c0e",
   "metadata": {},
   "source": [
    "## Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"This was the best movie I have ever seen!\", \"I really hated this film. It was slow and boring.\"]\n",
    "\n",
    "def predict(text, model):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    score = model.predict(padded, verbose=0)[0][0]\n",
    "    return score, \"Positive\" if score > 0.5 else \"Negative\"\n",
    "\n",
    "print(\"\\nTuned Model Predictions:\")\n",
    "for text in texts:\n",
    "    score, label = predict(text, best_model)\n",
    "    print(f\"{text[:50]}... -> {score:.4f} ({label})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
